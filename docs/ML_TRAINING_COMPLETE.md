# ML Training Plan â€” Complete Guide
## Phases, Tasks, Subtasks with Dependency Analysis, ROI & Cyclomatic Complexity

---

## ğŸ“‹ Table of Contents
1. Executive Summary
2. Phase 1: Zero-Train (MediaPipe) â€” RECOMMENDED
3. Phase 2: Light Finetune (Optional)
4. Phase 3: Full Training (Rare)
5. Phase 4: Integration & Deployment
6. Dependency Graph & Critical Path
7. ROI Analysis
8. Cyclomatic Complexity Breakdown
9. Execution Timeline
10. Quick Reference & Decision Tree

---

## ğŸ“Š Executive Summary

| Phase | Strategy | Cost | Time | ROI | Max CC | Risk |
|-------|----------|------|------|-----|--------|------|
| **1. Zero-Train** | MediaPipe | **$0** | 1-2h | ğŸ”¥ğŸ”¥ğŸ”¥ | 8 | ğŸŸ¢ Low |
| **2. Finetune** | MODNet | **$1-2** | 4-6h | ğŸ”¥ğŸ”¥ | 12 | ğŸŸ¡ Med |
| **3. Full Train** | From Scratch | **$50-200** | 20-40h | ğŸ”¥ | 15 | ğŸ”´ High |
| **4. Integration** | Deployment | **$0** | 2-3h | ğŸ”¥ğŸ”¥ | 8 | ğŸŸ¢ Low |

**Recommendation**: Start Phase 1. Only Phase 2 if IoU <0.85. Skip Phase 3.

**Path to Production**: 4.5-5.5 hours (Phases 1 + 4)

---

# PHASE 1: Zero-Train Reuse (RECOMMENDED)

## Overview
- **Status**: ğŸŸ¢ Ready to implement immediately
- **Cost**: $0 (no GPU)
- **Time**: 1-2 hours total
- **Risk**: Very low (proven Google technology)
- **ROI**: ğŸ”¥ğŸ”¥ğŸ”¥ (infinite â€” zero cost, 4/5 quality)
- **Quality Target**: IoU >0.85

### Why Phase 1 is Optimal
- MediaPipe models trained on millions of images (Google's standard)
- Zero GPU cost, instant deployment
- Covers 95% of use cases
- If insufficient, Phase 2 adds only $1-2

---

## Task 1.1: Environment Setup

**Complexity**: â­ (CC=1) | **Effort**: 30 min | **Dependencies**: None

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Install MediaPipe SDK (`pip install mediapipe`) | 1 | 2 min | â³ |
| Verify CUDA availability (if GPU available) | 2 | 5 min | â³ |
| Create test fixtures directory `tests/fixtures/` | 1 | 2 min | â³ |
| Download test images (20-30 diverse faces) | 1 | 15 min | â³ |
| Setup TensorBoard for metrics (optional) | 2 | 5 min | â³ |

**Deliverable**: `tests/fixtures/` with 20-30 test images ready for validation

---

## Task 1.2: MediaPipe Integration into core/segmenter.py

**Complexity**: â­â­ (CC=5) | **Effort**: 45 min | **Dependencies**: âœ… Task 1.1

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Load MediaPipe SelfieSegmentation model | 1 | 5 min | â³ |
| Configure model selection (general vs landscape) | 2 | 10 min | â³ |
| Implement `segment_hair()` with preprocess + inference | 3 | 20 min | â³ |
| Handle inference errors gracefully (fallback mask) | 4 | 10 min | â³ |

**Code Pattern** (CC=5):
```python
def segment_hair(image: np.ndarray) -> np.ndarray:
    if not validate(image):  # CC+1
        return fallback_mask()  # CC+1

    model = ModelCache.segmenter()

    if use_gpu():  # CC+1
        mask = gpu_inference(model, image)  # CC+1
    else:
        mask = cpu_inference(model, image)

    return mask  # CC+1
```

**Deliverable**: `core/segmenter.py` with real MediaPipe integration

---

## Task 1.3: Fixture Testing & Quality Validation

**Complexity**: â­â­ (CC=6) | **Effort**: 45 min | **Dependencies**: âœ… Task 1.1, 1.2

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Load 20+ fixtures (diverse skin tones, hair types) | 1 | 5 min | â³ |
| Run inference on each fixture | 2 | 10 min | â³ |
| Visual QA: inspect masks (acceptance: IoU >0.80) | 3 | 20 min | â³ |
| Log failures & categorize (lighting, hair, occlusion) | 4 | 10 min | â³ |

**Decision Logic** (CC=6):
```python
for fixture in fixtures:
    mask = segment_hair(fixture.image)
    iou = compute_iou(mask, fixture.gt_mask)

    if iou > 0.90:          # CC+1
        results['excellent'].append(fixture)  # CC+1
    elif iou > 0.85:        # CC+1
        results['good'].append(fixture)  # CC+1
    elif iou > 0.75:        # CC+1
        results['acceptable'].append(fixture)  # CC+1
    else:                   # CC+1
        failures.append(fixture)  # CC+1
```

**Success Criteria**: â‰¥95% of fixtures IoU >0.80, failures documented

---

## Task 1.4: Post-Processing Tuning

**Complexity**: â­â­â­ (CC=8) | **Effort**: 30 min | **Dependencies**: âœ… Task 1.3

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Configure blur radius for feathering | 1 | 5 min | â³ |
| Tune morphological operations (erosion/dilation) | 2 | 10 min | â³ |
| Implement edge anti-bleed (mask refinement) | 3 | 10 min | â³ |
| Benchmark visual results (side-by-side) | 2 | 5 min | â³ |

**Deliverable**: Optimized `postprocess_mask()` function with 10-20% visual quality improvement

---

## Task 1.5: Inference Benchmarking

**Complexity**: â­ (CC=2) | **Effort**: 15 min | **Dependencies**: âœ… Task 1.2 | **Can run parallel**: Yes

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Measure CPU inference time (50 runs, median) | 1 | 5 min | â³ |
| Measure GPU inference time (if available) | 1 | 5 min | â³ |
| Log p50, p95, p99 latencies | 2 | 5 min | â³ |

**Expected Results**:
- CPU: 100-150ms (target: <500ms)
- GPU: 30-50ms (target: <100ms)
- If >500ms CPU: optimize or require GPU

---

## Task 1.6: Deploy to Staging

**Complexity**: â­ (CC=1) | **Effort**: 15 min | **Dependencies**: âœ… Task 1.5, 1.4

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Update `model_registry.json` with MediaPipe v1.0 | 1 | 5 min | â³ |
| Deploy to staging environment | 1 | 5 min | â³ |
| Run E2E smoke test (mobile â†’ BFF â†’ ML API â†’ result) | 1 | 5 min | â³ |

**Success Criteria**:
- âœ… IoU >0.85 on 20+ fixtures
- âœ… Inference <500ms CPU / <100ms GPU
- âœ… E2E test: request received, image processed, response sent

### Phase 1 Summary
```
Critical Path: 1.1 â†’ 1.2 â†’ 1.3 â†’ 1.4 â†’ 1.5 â†’ 1.6
Parallel: 1.5 can run while 1.3-1.4 run
Total Effort: 2-2.5 hours
Cost: $0
Risk: ğŸŸ¢ Minimal
Result: Production-ready MediaPipe baseline
```

---

# PHASE 2: Light Finetune (IF NEEDED)

## Overview
- **Status**: ğŸŸ¡ Only if Phase 1 insufficient (IoU <0.85)
- **Cost**: $1-2 (spot GPU instance)
- **Time**: 4-6 hours (90 min GPU, rest CPU)
- **Risk**: Medium (GPU cost, dataset quality)
- **ROI**: ğŸ”¥ğŸ”¥ (3-4x quality improvement per dollar)
- **Quality Target**: IoU >0.90

### When to Escalate to Phase 2
- Phase 1 IoU <0.85 overall
- Edge cases failing >20% (curly hair, specific lighting, occlusion)
- Production metrics indicate insufficient mask quality

---

## Task 2.1: Dataset Preparation

**Complexity**: â­â­ (CC=4) | **Effort**: 60 min | **Dependencies**: None | **Can run parallel**: Yes

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Download Figaro-1k dataset (1050 images + masks) | 1 | 10 min | â³ |
| Visual QA: remove corrupted images | 2 | 15 min | â³ |
| Train/val/test split (70%/15%/15%) | 3 | 10 min | â³ |
| Pre-resize to 384x384, cache as .npy files | 3 | 20 min | â³ |
| Compute dataset statistics (mean, std) | 1 | 5 min | â³ |

**Dataset Pipeline** (CC=4):
```python
for img_path in figaro_paths:
    if is_corrupted(img_path):       # CC+1
        skip()
    else:
        resize_and_cache(img_path)    # CC+1

stats = compute_stats(dataset)        # CC+1
create_splits(dataset, [0.7, 0.15, 0.15])  # CC+1
```

**Deliverable**: `datasets/figaro-1k-cached/` with 1000+ images ready for training

---

## Task 2.2: GPU Provider Selection & Setup

**Complexity**: â­ (CC=2) | **Effort**: 15 min | **Dependencies**: None | **Can run parallel**: Yes

**Provider Comparison**:
| Provider | GPU | Cost/hr | Spot Cost | Recommendation |
|----------|-----|---------|-----------|----------------|
| Lambda Labs | A10 (24GB) | $0.75 | N/A | â­ Best price |
| Vast.ai | RTX 3090 | $0.30 | $0.15 | â­â­ Cheapest |
| GCP | T4 (16GB) | $0.35 | $0.11 | âœ… Spot pricing |
| Paperspace | A4000 | $0.76 | N/A | âœ… Good value |
| DigitalOcean | A10 | $3.00 | N/A | Simple, expensive |

**Recommendation**: Lambda Labs ($0.75/hr) for balance of price and reliability

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Select provider (Lambda Labs recommended) | 1 | 2 min | â³ |
| Create account, verify billing | 1 | 5 min | â³ |
| Test SSH access & GPU (`nvidia-smi`) | 1 | 5 min | â³ |
| Prepare SSH keys, security groups | 1 | 3 min | â³ |

**Cost Projection**:
- Setup + data validation: 30 min Ã— $0.75/hr = $0.38
- Training: 70 min Ã— $0.75/hr = $0.88
- Export + test: 20 min Ã— $0.75/hr = $0.25
- **Total**: ~$1.50 (1.25 GPU hours)

---

## Task 2.3: Model Baseline Selection

**Complexity**: â­ (CC=1) | **Effort**: 10 min | **Dependencies**: None

**Model Comparison**:
| Model | Params | CPU Speed | GPU Speed | Quality | Finetune |
|-------|--------|-----------|-----------|---------|----------|
| MediaPipe | 2M | 100ms | 30ms | â­â­â­â­ | N/A |
| MODNet (MobileNetV2) | 6.5M | 200ms | 50ms | â­â­â­â­ | Low âœ… |
| BiSeNet V2 | 5.6M | 180ms | 45ms | â­â­â­â­â­ | Medium |
| DeepLabV3+ | 5.8M | 220ms | 60ms | â­â­â­â­ | Medium |
| U-Net (ResNet34) | 24M | 400ms | 100ms | â­â­â­â­â­ | High |

**Recommendation**: **MODNet (MobileNetV2)** â€” best quality/speed/effort balance

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Compare MODNet vs BiSeNet vs DeepLabV3+ | 2 | 5 min | â³ |
| Select MODNet (MobileNetV2) | 1 | 2 min | â³ |
| Download pre-trained checkpoint | 1 | 3 min | â³ |

---

## Task 2.4: Training on GPU Instance

**Complexity**: â­â­â­â­ (CC=12) | **Effort**: 90 min GPU | **Dependencies**: âœ… Task 2.1, 2.2, 2.3

### Subtask 2.4a: Data Loading & Validation (30 min)

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Load .npy cached dataset | 1 | 2 min | â³ |
| Create DataLoader (4 workers + pin_memory) | 2 | 5 min | â³ |
| Verify data shape and batch integrity | 3 | 5 min | â³ |
| Inspect augmentation pipeline (albumentations) | 4 | 10 min | â³ |
| Compute class weights (hair vs background) | 2 | 5 min | â³ |

**Albumentations Pipeline**:
```python
transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.3),
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
    A.GaussianBlur(blur_limit=3, p=0.2),
    A.RandomBrightnessContrast(p=0.3),
    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),
    A.Resize(384, 384),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
```

### Subtask 2.4b: Training Loop (60 min GPU)

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Load MODNet checkpoint, freeze backbone | 1 | 3 min | â³ |
| Configure mixed precision (FP16) + GradScaler | 2 | 5 min | â³ |
| Setup gradient accumulation (steps=4) | 3 | 5 min | â³ |
| Define loss (BCE + Dice) with class weighting | 2 | 5 min | â³ |
| Setup optimizer (AdamW) + LR scheduler (cosine) | 4 | 5 min | â³ |
| Configure early stopping (patience=3) | 2 | 3 min | â³ |
| **Training loop**: 5 epochs | 8 | 60-75 min | â³ |
| Save best checkpoint (by val IoU) | 1 | 2 min | â³ |

**Training Hyperparameters**:
```python
config = {
    'epochs': 5,              # Light finetune
    'batch_size': 4,          # Fits in 16GB GPU with FP16
    'accumulation_steps': 4,  # Effective batch=16
    'lr': 1e-4,               # Conservative
    'weight_decay': 1e-5,
    'optimizer': 'AdamW',
    'scheduler': 'cosine',
    'warmup_steps': 100,
    'early_stopping_patience': 3,
    'mixed_precision': True,  # FP16 â†’ 2x speedup
    'gradient_clip': 1.0,
}
```

**Training Loop CC Breakdown** (CC=8):
```python
for epoch in range(5):  # CC+1
    for batch_idx, (images, masks) in enumerate(train_loader):
        if should_accumulate(batch_idx):  # CC+1
            loss = forward_pass(images, masks)  # CC+1
            loss.backward()
        else:  # CC+1
            optimizer.step()
            optimizer.zero_grad()

    val_loss, val_iou = validate()  # CC+1

    if early_stop_check():  # CC+1
        break  # CC+1

    if val_loss < best_loss:  # CC+1
        save_checkpoint()  # CC+1
```

**Expected Timeline**:
- Epoch 1: 15 min (first, includes overhead)
- Epochs 2-5: 12 min each
- Early stopping likely at epoch 3-4: saves 30-50% GPU time
- Total: 60-75 min GPU

---

## Task 2.5: Export & Evaluation

**Complexity**: â­â­ (CC=5) | **Effort**: 25 min | **Dependencies**: âœ… Task 2.4

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Load best checkpoint | 1 | 2 min | â³ |
| Compute metrics (IoU, F1, precision, recall) | 3 | 8 min | â³ |
| Generate visual comparisons (20 samples) | 2 | 5 min | â³ |
| Export to TorchScript (.pt) | 2 | 3 min | â³ |
| Export to ONNX (.onnx) | 2 | 3 min | â³ |
| Update `model_registry.json` | 1 | 3 min | â³ |

**Evaluation Decision Tree** (CC=5):
```python
def evaluate_model(checkpoint):
    metrics = compute_metrics(checkpoint)  # CC+1

    if metrics['iou'] > 0.90:    # CC+1
        print("PRODUCTION READY")
        return APPROVED           # CC+1
    elif metrics['iou'] > 0.85:  # CC+1
        print("ACCEPTABLE, MONITOR")
        return CONDITIONAL        # CC+1
    else:
        print("REJECT, RETRAIN")
        return REJECTED
```

**model_registry.json**:
```json
{
  "hair_segmenter": {
    "version": "1.1",
    "created_at": "2025-01-15T10:30:00Z",
    "base_model": "MODNet_MobileNetV2",
    "dataset": "Figaro-1k + augmentation (1000 samples)",
    "metrics": {
      "val_iou": 0.92,
      "val_f1": 0.94,
      "inference_time_cpu": "280ms",
      "inference_time_gpu": "45ms"
    },
    "file": "hair_segmenter_v1.1.pt",
    "size_mb": 26.4
  }
}
```

---

## Task 2.6: Cost Minimization

**Complexity**: â­ (CC=1) | **Effort**: 5 min | **Dependencies**: âœ… Task 2.5

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Download model to local machine | 1 | 2 min | â³ |
| Terminate GPU instance immediately | 1 | 1 min | â³ |
| Verify termination in provider console | 1 | 2 min | â³ |

**Critical**: Terminate instance immediately or lose $15-20 per idle hour.

### Phase 2 Summary
```
Critical Path: 2.1 â†’ 2.2 â†’ 2.3 â†’ 2.4 â†’ 2.5 â†’ 2.6
Parallel: 2.2, 2.3 can run parallel with 2.1
Total Effort: 4-6 hours (1.5h CPU + 1.5h waiting + 90 min GPU)
Cost: $0.94-1.50 (1.25-2 GPU hours)
Risk: ğŸŸ¡ Medium
Result: MODNet v1.1 with IoU >0.90
```

---

# PHASE 3: Full Training (RARE)

## Overview
- **Status**: ğŸ”´ Only if Phase 2 still insufficient (IoU <0.90)
- **Cost**: $50-200 (10-20 GPU hours)
- **Time**: 20-40 hours
- **Risk**: High (expensive, long, uncertain)
- **ROI**: ğŸ”¥ (low â€” diminishing returns)
- **When**: Very rare, only if extreme quality needed

### Not Recommended for MVP
- Diminishing returns: 90â†’95 IoU improvement costs $50-200
- Phase 1 + Phase 2 covers 99% of use cases
- Only if production traffic reveals systematic failures

---

## Task 3.1: Dataset Expansion

**Complexity**: â­â­ (CC=4) | **Effort**: 170 min

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Collect additional datasets (CelebA-HQ, LFW) | 2 | 60 min | â³ |
| Annotate custom images (manual hair masks) | 3 | 90 min | â³ |
| Merge, deduplicate, create 5000+ samples | 2 | 15 min | â³ |
| Validate balanced distribution | 1 | 5 min | â³ |

---

## Task 3.2: Full Training from Scratch

**Complexity**: â­â­â­â­ (CC=15) | **Effort**: 600-1200 min GPU

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Train DeepLabV3+ from scratch | 15 | 600-1200 min | â³ |
| Monitor loss curves & validation metrics | 2 | 30 min | â³ |
| Implement learning rate decay & warmup | 3 | 30 min | â³ |

---

## Task 3.3: Post-Training Analysis

**Complexity**: â­â­â­ (CC=7) | **Effort**: 120 min

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Hyperparameter sensitivity analysis | 3 | 60 min | â³ |
| Error analysis (worst cases) | 2 | 30 min | â³ |
| Model compression & quantization | 4 | 30 min | â³ |

### Phase 3 Summary
```
Total Effort: 20-40 hours
Cost: $50-200 (10-20 GPU hours)
Risk: ğŸ”´ High
Result: Highest possible quality (IoU >0.95)
Recommendation: AVOID for MVP
```

---

# PHASE 4: Integration & Production Deployment

## Overview
- **Status**: ğŸŸ¢ Required after Phase 1 or Phase 2
- **Cost**: $0 (deployment only)
- **Time**: 2-3 hours
- **Risk**: ğŸŸ¢ Low (rollback plan in place)
- **ROI**: ğŸ”¥ğŸ”¥ (enables production use, risk-controlled)

---

## Task 4.1: Model Versioning & Registry

**Complexity**: â­ (CC=2) | **Effort**: 15 min | **Dependencies**: Phase 1 or 2 complete

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Create `model_registry.json` with metadata | 1 | 5 min | â³ |
| Store model in Git LFS or S3 with checksums | 1 | 5 min | â³ |
| Document training command & hyperparameters | 1 | 5 min | â³ |

---

## Task 4.2: Update segmenter.py with Production Model

**Complexity**: â­â­ (CC=5) | **Effort**: 30 min | **Dependencies**: âœ… Task 4.1

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Replace stub with real model inference | 2 | 10 min | â³ |
| Update ModelCache to load production model | 3 | 10 min | â³ |
| Test integration: image â†’ mask â†’ output | 1 | 5 min | â³ |
| Add version logging to every request | 2 | 5 min | â³ |

---

## Task 4.3: A/B Testing Setup

**Complexity**: â­â­â­ (CC=8) | **Effort**: 45 min | **Dependencies**: âœ… Task 4.2

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Implement feature flag (10%â†’50%â†’100%) | 2 | 15 min | â³ |
| Setup metrics collection | 3 | 15 min | â³ |
| Define rollback thresholds | 1 | 5 min | â³ |
| Test rollback procedure | 2 | 10 min | â³ |

**A/B Testing Decision Tree** (CC=8):
```python
def should_rollout(metrics):
    if metrics['error_rate'] > 0.02:      # CC+1
        return ROLLBACK                    # CC+1
    elif metrics['latency_p95'] > 1000:   # CC+1
        return ROLLBACK                    # CC+1
    elif current_traffic == 0.10:         # CC+1
        return INCREASE_TO_50              # CC+1
    elif current_traffic == 0.50:         # CC+1
        return INCREASE_TO_100             # CC+1
    else:
        return MONITOR                     # CC+1
```

**Rollout Schedule**:
- Week 1: 10% traffic (monitor for errors)
- Week 2: 50% traffic (if metrics OK)
- Week 3: 100% traffic (full deployment)

**Rollback Thresholds**:
- Error rate >2% â†’ ROLLBACK immediately
- Latency p95 >1000ms â†’ ROLLBACK
- IoU metric shows regression â†’ ROLLBACK

---

## Task 4.4: Monitoring & Alerting

**Complexity**: â­â­â­ (CC=7) | **Effort**: 30 min | **Dependencies**: âœ… Task 4.3

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Setup TensorBoard dashboard | 2 | 10 min | â³ |
| Create alerts for error/latency thresholds | 3 | 10 min | â³ |
| Log segmentation failures for analysis | 2 | 5 min | â³ |
| Setup Slack/email notifications | 2 | 5 min | â³ |

**Metrics to Monitor**:
- Inference latency (p50, p95, p99)
- Error rate (segmentation failures per 1000 requests)
- Model version running on each instance
- Edge case categorization (what fails)

---

## Task 4.5: Documentation & Handoff

**Complexity**: â­ (CC=1) | **Effort**: 20 min | **Dependencies**: âœ… Task 4.4

| Subtask | CC | Time | Status |
|---------|-----|------|--------|
| Update README.md with deployed version | 1 | 5 min | â³ |
| Document rollback procedure | 1 | 5 min | â³ |
| Create incident response playbook | 2 | 10 min | â³ |

### Phase 4 Summary
```
Critical Path: 4.1 â†’ 4.2 â†’ 4.3 â†’ 4.4 â†’ 4.5
Parallel: 4.3, 4.4 can run after 4.2
Total Effort: 2-3 hours
Cost: $0
Risk: ğŸŸ¢ Minimal
Result: Safe production deployment with rollback
```

---

# ğŸ“Š Dependency Graph & Critical Path

## Dependency Flow

```
PHASE 1                          PHASE 2 (Optional)         PHASE 4
â”€â”€â”€â”€â”€â”€â”€â”€                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€
1.1 Env
 â†“
1.2 Integration    2.1 Data â—„â”€â”
 â†“                  â†“         â”‚ (Parallel)
1.3 Testing â—„â”€â”€â”€â”€  2.2 GPU â”€â”€â”¤
 â†“                  â†“         â”‚
1.4 Post-Proc      2.3 Model â—„â”˜
 â†“
1.5 Bench â—„â”€â”€â”€â”€â”€â”€â”€â”€â”
 â†“                 â”‚
1.6 Deploy â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â†’ 4.1 Versioning
                             â†“
                           4.2 Integration
                             â†“
                           4.3 A/B Testing
                             â†“
                           4.4 Monitoring
                             â†“
                           4.5 Documentation
                             â†“
                          ğŸ‰ PRODUCTION
```

## Critical Paths to Production

**Fast Track (Phase 1 only)**: 2.5-3 hours
```
1.1 â†’ 1.2 â†’ 1.3 â†’ 1.4 â†’ 1.5 â†’ 1.6 â†’ 4.1 â†’ 4.2 â†’ 4.3||4.4 â†’ 4.5
```

**With Optional Finetune (Phase 1+2)**: 5-7 hours work + 90 min GPU wait
```
2.1||2.2||2.3 (1h) â†’ 2.4 (90 min GPU) â†’ 2.5 â†’ 2.6 (5 min)
Parallel: 1.1 â†’ 1.2 â†’ 1.3 â†’ 1.4 â†’ 1.5 â†’ 1.6 (can run while 2.4 trains)
Then: 4.1 â†’ 4.2 â†’ 4.3||4.4 â†’ 4.5 (2-3 hours)
```

---

# ğŸ“ˆ ROI Analysis

## Cost vs Quality Trade-off

| Phase | Cost | Quality | ROI Score | Decision |
|-------|------|---------|-----------|----------|
| Phase 1 (Zero-Train) | $0 | 4/5 | ğŸ”¥ğŸ”¥ğŸ”¥ INFINITE | âœ… ALWAYS START |
| Phase 2 (Finetune) | $1-2 | 4.5-5/5 | ğŸ”¥ğŸ”¥ (3-4x per $) | âœ… IF IoU <0.85 |
| Phase 3 (Full Train) | $50-200 | 5/5 | ğŸ”¥ (1x per $) | âŒ AVOID |
| Phase 4 (Deploy) | $0 | N/A | ğŸ”¥ğŸ”¥ REQUIRED | âœ… ALWAYS DO |

## Cost Breakdown (Phase 1)
```
Environment setup:       $0
Integration:             $0
Testing & validation:    $0
Post-processing:         $0
Benchmarking:            $0
Deployment:              $0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                   $0 â† FREE
```

## Cost Breakdown (Phase 2)
```
Data preparation:        $0 (CPU work at home)
GPU setup:               $0
Model selection:         $0
Training (90 min GPU):   1.25 Ã— $0.75 = $0.94
Export & evaluation:     $0.19 (15 min GPU)
Cost minimization:       $0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                   $1.13 â† ~$1-2
```

## Quality Improvement

```
Phase 1:  IoU 0.85  â†’ Baseline (4/5 stars)
Phase 2:  IoU 0.92  â†’ +0.07 improvement (4.5-5/5 stars)
Phase 3:  IoU 0.95  â†’ +0.03 improvement (5/5 stars, overkill)
```

---

# ğŸ” Cyclomatic Complexity Analysis

## CC by Task

| Phase | Task | CC | Risk | Testability |
|-------|------|-----|------|------------|
| 1 | 1.1 Setup | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 1 | 1.2 Integration | 5 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 1 | 1.3 Testing | 6 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 1 | 1.4 Post-Process | 8 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 1 | 1.5 Benchmark | 2 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 1 | 1.6 Deploy | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 2 | 2.1 Data Prep | 4 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 2 | 2.2 GPU Setup | 2 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 2 | 2.3 Model Select | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 2 | 2.4 Training | 12 | ğŸ”´ High | ğŸŸ¡ Fair |
| 2 | 2.5 Export | 5 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 2 | 2.6 Cost Min | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 4 | 4.1 Version | 2 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 4 | 4.2 Integration | 5 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 4 | 4.3 A/B Test | 8 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 4 | 4.4 Monitor | 7 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 4 | 4.5 Docs | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |

## Complexity Legend

- **â­ (CC 1-2)**: Simple, trivial â€” basic if/for statements
- **â­â­ (CC 3-5)**: Low-moderate â€” straightforward control flow
- **â­â­â­ (CC 6-8)**: Moderate â€” multiple branches, needs attention
- **â­â­â­â­ (CC 9+)**: Complex â€” high risk, needs extensive testing

## High-Risk Areas

**Task 2.4 Training (CC=12)**:
- Nested loops (epochs, batches)
- Multiple conditional branches (early stopping, accumulation, precision)
- Multiple branches in validation logic
- Mitigation: Use well-tested frameworks (PyTorch), add assertions, monitor metrics

---

# ğŸš€ Recommended Execution Timeline

## Week 1: Phase 1 (Zero-Train to Production)

```
Monday-Tuesday:   Tasks 1.1-1.2 (Environment + Integration)
                  Effort: 75 min total
                  Cost: $0

Wednesday:        Tasks 1.3-1.4 (Testing + Post-Processing)
                  Effort: 75 min total
                  Cost: $0

Thursday:         Tasks 1.5-1.6 (Benchmarking + Deploy)
                  Effort: 30 min total
                  Cost: $0

TOTAL WEEK 1:     2-2.5 hours â†’ MediaPipe in staging
                  Cost: $0
                  Risk: Minimal
```

## Week 2: Phase 2 (IF IoU <0.85)

```
Only if Phase 1 failed acceptance criteria

Monday-Tuesday:   Tasks 2.1-2.3 (Data Prep + GPU Setup + Model Select)
                  Effort: 85 min CPU
                  Cost: $0

Wednesday-Thursday: Task 2.4 (Training)
                  Effort: 90 min GPU (can run overnight)
                  Cost: $0.94

Friday:           Tasks 2.5-2.6 (Export + Shutdown)
                  Effort: 30 min
                  Cost: $0.25

TOTAL WEEK 2:     4-6 hours â†’ MODNet in staging
                  Cost: $1.19
                  Risk: Low
```

## Week 3: Phase 4 (Production Integration)

```
Monday:           Tasks 4.1-4.2 (Versioning + Integration)
                  Effort: 45 min
                  Cost: $0

Tuesday:          Task 4.3 (A/B Testing Setup)
                  Effort: 45 min
                  Cost: $0

Wednesday:        Task 4.4 (Monitoring)
                  Effort: 30 min
                  Cost: $0

Thursday:         Task 4.5 (Documentation)
                  Effort: 20 min
                  Cost: $0

TOTAL WEEK 3:     2.5-3 hours â†’ Safe rollout procedure
                  Cost: $0
                  Risk: Minimal

FINAL: 10% â†’ 50% â†’ 100% traffic rollout with rollback ready
```

---

# ğŸ¯ Decision Tree & Success Criteria

## Go/No-Go Criteria by Phase

```
START
  â†“
PHASE 1: Zero-Train (Mandatory)
  â”œâ”€ Run Tasks 1.1-1.6
  â”œâ”€ Check: IoU >0.85? âœ… GO to Phase 4
  â””â”€ Check: IoU <0.85? âš ï¸ ESCALATE to Phase 2

PHASE 2: Finetune (Conditional)
  â”œâ”€ Check: IoU <0.85 failures <20%? âœ… GO to Phase 4
  â”œâ”€ Check: Cost will exceed $10? âŒ CANCEL
  â”œâ”€ Run Tasks 2.1-2.6
  â”œâ”€ Check: IoU >0.90? âœ… GO to Phase 4
  â””â”€ Check: IoU <0.90? ğŸ”´ RARE â†’ Phase 3 or accept Phase 2

PHASE 4: Integration (Required)
  â”œâ”€ Run Tasks 4.1-4.5
  â”œâ”€ Check: Error rate <2%? âœ… PROCEED with rollout
  â”œâ”€ Check: Latency p95 <1000ms? âœ… PROCEED
  â”œâ”€ Week 1: 10% traffic â†’ âœ… metrics OK?
  â”œâ”€ Week 2: 50% traffic â†’ âœ… metrics OK?
  â””â”€ Week 3: 100% traffic â†’ ğŸ‰ PRODUCTION LIVE

DONE: Model in production, monitoring active, rollback ready
```

## Success Criteria

### Phase 1 Exit Criteria
- âœ… IoU >0.85 on 20+ diverse test fixtures
- âœ… Inference latency <500ms CPU / <100ms GPU
- âœ… Zero segmentation crashes
- âœ… E2E test: request â†’ processed image â†’ response

### Phase 2 Exit Criteria (if needed)
- âœ… Val IoU >0.90 on test set
- âœ… Training completed in <120 min
- âœ… Model checkpoints versioned and backed up
- âœ… Inference latency within acceptable range

### Phase 4 Exit Criteria
- âœ… Error rate <2% during rollout
- âœ… Latency p95 <1000ms sustained
- âœ… Rollback procedure tested successfully
- âœ… Monitoring alerts configured and tested

---

# ğŸ“‹ Quick Reference Checklist

## Phase 1 (2-2.5 hours, $0)

```
â–¡ 1.1 Install MediaPipe, create fixtures dir          (30 min)
â–¡ 1.2 Integrate MediaPipe into segmenter.py           (45 min)
â–¡ 1.3 Test on 20+ fixtures, validate IoU >0.85       (45 min)
â–¡ 1.4 Tune post-processing (blur, feathering)         (30 min)
â–¡ 1.5 Benchmark CPU/GPU inference time                (15 min)
â–¡ 1.6 Deploy to staging, run E2E test                 (15 min)

âœ… Exit: IoU >0.85, latency <500ms, E2E passes
```

## Phase 2 IF NEEDED (4-6 hours, $1-2)

```
â–¡ 2.1 Download Figaro-1k, prepare dataset             (60 min)
â–¡ 2.2 Select GPU provider (Lambda Labs)               (15 min)
â–¡ 2.3 Choose MODNet baseline                          (10 min)
â–¡ 2.4 Run training on GPU (can be overnight)          (90 min GPU)
â–¡ 2.5 Export model, compute metrics                   (25 min)
â–¡ 2.6 Terminate GPU instance immediately              (5 min)

âœ… Exit: IoU >0.90, model versioned, cost <$2
```

## Phase 4 REQUIRED (2-3 hours, $0)

```
â–¡ 4.1 Create model_registry.json                      (15 min)
â–¡ 4.2 Update segmenter.py with production model       (30 min)
â–¡ 4.3 Implement feature flag & A/B testing            (45 min)
â–¡ 4.4 Setup monitoring & alerts                       (30 min)
â–¡ 4.5 Update docs & create runbooks                   (20 min)

âœ… Exit: Safe rollout procedure ready, rollback tested
```

---

# ğŸ”— Related Documents

- `docs/ML_REQUIREMENTS.md` â€” ML API specifications
- `docs/CONTRIBUTING.md` â€” PR and deployment procedures
- `scripts/train_hair_segmentation.sh` â€” Automated training script
- `docker/Dockerfile.training` â€” GPU environment dockerfile

---

**Document Version**: 2.0 (Complete)
**Last Updated**: 2025-12-20
**Owner**: ML Team
**Review Cycle**: After each model deployment
