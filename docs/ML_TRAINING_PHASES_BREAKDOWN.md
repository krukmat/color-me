# ML Training Plan â€” Phases, Tasks & Subtasks Breakdown
## With Dependency Analysis, ROI, and Cyclomatic Complexity

---

## ğŸ“Š Executive Summary

| Phase | Strategy | Est. Cost | Est. Time | ROI | Complexity |
|-------|----------|-----------|-----------|-----|------------|
| **Phase 1: Zero-Train (MediaPipe)** | Pre-trained model | **$0** | **1-2h** | ğŸ”¥ğŸ”¥ğŸ”¥ HIGH | â­ LOW (3) |
| **Phase 2: Light Finetune (Optional)** | MODNet + Figaro-1k | **$1-2** | **4-6h** | ğŸ”¥ğŸ”¥ MEDIUM | â­â­ MEDIUM (7) |
| **Phase 3: Full Training (Rare)** | From scratch | **$50-200** | **20-40h** | ğŸ”¥ LOW | â­â­â­ HIGH (12+) |
| **Phase 4: Integration & Monitoring** | Prod deployment | **$0** | **2-3h** | ğŸ”¥ğŸ”¥ MEDIUM | â­â­ MEDIUM (8) |

---

## PHASE 1: Zero-Train Reuse (RECOMMENDED)

### Overview
- **Status**: ğŸŸ¢ Ready to implement immediately
- **Cost**: $0 (no GPU)
- **Time**: 1-2 hours
- **Risk**: Very low (proven technology)
- **ROI**: Extremely high (1x effort = 4x quality gain vs custom training)

### Task 1.1: Environment Setup
**Complexity**: â­ (CC = 1)
**ROI**: ğŸ”¥ğŸ”¥ğŸ”¥ (prerequisite for all downstream tasks)
**Effort**: 30 min

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Install MediaPipe SDK (`pip install mediapipe`) | â­ (1) | None | 2 min | â³ |
| Verify CUDA availability (if GPU available) | â­ (2) | None | 5 min | â³ |
| Create test fixtures directory `tests/fixtures/` | â­ (1) | None | 2 min | â³ |
| Download test images (20-30 diverse faces) | â­ (1) | None | 15 min | â³ |
| Setup TensorBoard for metrics (optional) | â­ (2) | None | 5 min | â³ |

**Dependencies**: None (standalone task)

**Rollback**: `pip uninstall mediapipe`, delete fixtures dir

---

### Task 1.2: MediaPipe Integration into core/segmenter.py
**Complexity**: â­â­ (CC = 5)
**ROI**: ğŸ”¥ğŸ”¥ğŸ”¥ (enables full pipeline)
**Effort**: 45 min
**Dependencies**: Task 1.1 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Load MediaPipe SelfieSegmentation model | â­ (1) | 1.1 | 5 min | â³ |
| Configure model selection (general vs landscape) | â­â­ (2) | 1.1 | 10 min | â³ |
| Implement `segment_hair()` function with preprocess + inference | â­â­ (3) | 1.1 | 20 min | â³ |
| Handle inference errors gracefully (fallback mask) | â­â­ (4) | 1.1 | 10 min | â³ |

**Code Structure** (CC = 5):
```python
def segment_hair(image):  # CC=1
    if not validate(image):  # CC=2
        return fallback()  # CC=3

    model = ModelCache.segmenter()  # CC=3

    if use_gpu():  # CC=4
        mask = gpu_infer(model, image)  # CC=5
    else:
        mask = cpu_infer(model, image)

    return postprocess(mask)
```

---

### Task 1.3: Fixture Testing & Quality Validation
**Complexity**: â­â­ (CC = 6)
**ROI**: ğŸ”¥ğŸ”¥ğŸ”¥ (prevents bad model deployment)
**Effort**: 45 min
**Dependencies**: Task 1.1 âœ…, Task 1.2 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Load 20+ test fixtures (diverse skin tones, hair types) | â­ (1) | 1.1 | 5 min | â³ |
| Run inference on each fixture | â­â­ (2) | 1.2 | 10 min | â³ |
| Visual QA: inspect masks (acceptance: IoU >0.80) | â­â­ (3) | 1.2 | 20 min | â³ |
| Log failures and categorize (lighting, hair type, occlusion) | â­â­â­ (4) | 1.2 | 10 min | â³ |

**Decision Logic** (CC = 6):
```python
for fixture in fixtures:
    mask = segment_hair(fixture.image)
    iou = compute_iou(mask, fixture.gt_mask)

    if iou > 0.90:  # CC+1
        print("EXCELLENT")
    elif iou > 0.85:  # CC+1
        print("GOOD, monitor")
    elif iou > 0.75:  # CC+1
        print("ACCEPTABLE, edge case")
    else:  # CC+1
        failures.append(fixture.name)  # CC+1
        categorize_failure(fixture)  # CC+1
```

---

### Task 1.4: Post-Processing Tuning
**Complexity**: â­â­â­ (CC = 8)
**ROI**: ğŸ”¥ğŸ”¥ (visual quality improvement 10-20%)
**Effort**: 30 min
**Dependencies**: Task 1.3 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Configure blur radius for feathering | â­ (1) | 1.3 | 5 min | â³ |
| Tune morphological operations (erosion/dilation) | â­â­ (2) | 1.3 | 10 min | â³ |
| Implement edge anti-bleed (mask refinement) | â­â­â­ (3) | 1.3 | 10 min | â³ |
| Benchmark visual results (side-by-side comparison) | â­â­ (2) | 1.3 | 5 min | â³ |

---

### Task 1.5: Inference Benchmarking
**Complexity**: â­ (CC = 2)
**ROI**: ğŸ”¥ğŸ”¥ (informs scaling decisions)
**Effort**: 15 min
**Dependencies**: Task 1.2 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Measure CPU inference time (50 runs, take median) | â­ (1) | 1.2 | 5 min | â³ |
| Measure GPU inference time (if available) | â­ (1) | 1.2 | 5 min | â³ |
| Log p50, p95, p99 latencies | â­â­ (2) | 1.2 | 5 min | â³ |

**Expected Results**:
- CPU: 100-150ms
- GPU: 30-50ms
- If >500ms on CPU: optimize or use GPU requirement

---

### Task 1.6: Deployment to Staging
**Complexity**: â­ (CC = 1)
**ROI**: ğŸ”¥ğŸ”¥ (enables end-to-end testing)
**Effort**: 15 min
**Dependencies**: Task 1.5 âœ…, Task 1.4 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Update `model_registry.json` with MediaPipe version | â­ (1) | 1.4 | 5 min | â³ |
| Deploy to staging environment | â­ (1) | 1.4 | 5 min | â³ |
| Run smoke test: mobile â†’ BFF â†’ ML API â†’ response | â­ (1) | 1.4 | 5 min | â³ |

---

## Summary: Phase 1 (Zero-Train)

```
Total Effort: 2-2.5 hours
Total Cost: $0
Max Complexity: CC=8 (post-processing tuning)
Critical Path: 1.1 â†’ 1.2 â†’ 1.3 â†’ 1.5 â†’ 1.6
Parallel Tasks: 1.1 (environment), 1.5 (benchmarking after 1.2 done)
ROI: ğŸ”¥ğŸ”¥ğŸ”¥ (immediate production deployment, zero cost)
```

**Success Criteria**:
- âœ… IoU >0.85 on 20+ fixtures
- âœ… Inference <500ms CPU / <100ms GPU
- âœ… End-to-end test passes (mobile request â†’ processed image)

---

## PHASE 2: Light Finetune (IF NEEDED)

### Overview
- **Status**: ğŸŸ¡ Only if Phase 1 insufficient (IoU <0.85)
- **Cost**: $1-2 (spot GPU instance)
- **Time**: 4-6 hours
- **Risk**: Medium (GPU cost, dataset quality)
- **ROI**: Medium (improves quality 5-10%)
- **When**: Edge case failures >20% of production traffic

### Task 2.1: Dataset Preparation
**Complexity**: â­â­ (CC = 4)
**ROI**: ğŸ”¥ğŸ”¥ (determines training quality ceiling)
**Effort**: 1 hour
**Dependencies**: None (parallel with Phase 1)

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Download Figaro-1k dataset (1050 images + masks) | â­ (1) | None | 10 min | â³ |
| Visual QA: remove corrupted images | â­â­ (2) | None | 15 min | â³ |
| Train/val/test split (70% / 15% / 15%) | â­â­ (3) | None | 10 min | â³ |
| Pre-resize to 384x384, cache as .npy files | â­â­ (3) | None | 20 min | â³ |
| Compute dataset statistics (mean, std normalization) | â­ (1) | None | 5 min | â³ |

**Complexity Analysis** (CC = 4):
```python
def prepare_dataset():
    for img_path in figaro_paths:  # CC+1
        if is_corrupted(img_path):  # CC+1
            skip()
        else:
            resize_and_cache(img_path)  # CC+1

    stats = compute_stats(dataset)  # CC+1
```

---

### Task 2.2: GPU Provider Selection & Setup
**Complexity**: â­ (CC = 2)
**ROI**: ğŸ”¥ğŸ”¥ğŸ”¥ (5x cost reduction if spot selected)
**Effort**: 15 min
**Dependencies**: None (parallel with 2.1)

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Select provider (Lambda Labs $0.75/hr recommended) | â­ (1) | None | 2 min | â³ |
| Create account and verify billing | â­ (1) | None | 5 min | â³ |
| Test SSH access and GPU availability (`nvidia-smi`) | â­ (1) | None | 5 min | â³ |
| Prepare SSH keys and security groups | â­ (1) | None | 3 min | â³ |

---

### Task 2.3: Model Baseline Selection
**Complexity**: â­ (CC = 1)
**ROI**: ğŸ”¥ğŸ”¥ (model choice affects all downstream training)
**Effort**: 10 min
**Dependencies**: None

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Compare MODNet vs BiSeNet vs DeepLabV3+ (quality/speed tradeoff) | â­â­ (2) | None | 5 min | â³ |
| Select MODNet (MobileNetV2 backbone) | â­ (1) | None | 2 min | â³ |
| Download pre-trained checkpoint | â­ (1) | None | 3 min | â³ |

---

### Task 2.4: Training Notebook Execution (GPU Instance)
**Complexity**: â­â­â­â­ (CC = 12)
**ROI**: ğŸ”¥ğŸ”¥ (core ML work, expensive)
**Effort**: 1.5-2 hours GPU time
**Dependencies**: Task 2.1 âœ…, Task 2.2 âœ…, Task 2.3 âœ…

**Notebook 2.4a: Data Loading & Validation** (30 min)
| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Load .npy cached dataset | â­ (1) | 2.1 | 2 min | â³ |
| Create DataLoader with 4 workers + pin_memory | â­â­ (2) | 2.1 | 5 min | â³ |
| Verify data shape and batch integrity | â­â­ (3) | 2.1 | 5 min | â³ |
| Inspect augmentation pipeline (albumentations) | â­â­â­ (4) | 2.1 | 10 min | â³ |
| Compute class weights (hair vs background imbalance) | â­â­ (2) | 2.1 | 5 min | â³ |

**Notebook 2.4b: Model Setup & Training Loop** (80-100 min GPU)
| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Load MODNet checkpoint, freeze backbone | â­ (1) | 2.3 | 3 min | â³ |
| Configure mixed precision (FP16) + GradScaler | â­â­ (2) | 2.3 | 5 min | â³ |
| Setup gradient accumulation (steps=4) | â­â­â­ (3) | 2.3 | 5 min | â³ |
| Define loss (BCE + Dice) with class weighting | â­â­ (2) | 2.3 | 5 min | â³ |
| Setup optimizer (AdamW) + LR scheduler (cosine) | â­â­â­ (4) | 2.3 | 5 min | â³ |
| Configure early stopping (patience=3) | â­â­ (2) | 2.3 | 3 min | â³ |
| **Training loop**: 5 epochs Ã— 12-15 min each | â­â­â­â­ (8) | 2.3 | 60-75 min | â³ |
| Save best checkpoint (by val IoU) | â­ (1) | 2.3 | 2 min | â³ |
| Log metrics to TensorBoard | â­â­ (2) | 2.3 | 2 min | â³ |

**Training Loop CC Breakdown** (CC = 8):
```python
for epoch in range(5):  # CC+1
    for batch_idx, (images, masks) in enumerate(train_loader):
        if should_accumulate(batch_idx):  # CC+1
            loss = forward_pass(images, masks)  # CC+1
            loss.backward()
        else:  # CC+1
            optimizer.step()
            optimizer.zero_grad()

    val_loss, val_iou = validate()  # CC+1

    if early_stop_check():  # CC+1
        break  # CC+1

    if val_loss < best_loss:  # CC+1
        save_checkpoint()  # CC+1
```

---

### Task 2.5: Evaluation & Export
**Complexity**: â­â­ (CC = 5)
**ROI**: ğŸ”¥ğŸ”¥ (validates quality, enables deployment)
**Effort**: 20-30 min
**Dependencies**: Task 2.4 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Load best checkpoint | â­ (1) | 2.4 | 2 min | â³ |
| Compute metrics (IoU, F1, precision, recall) on val/test sets | â­â­ (3) | 2.4 | 8 min | â³ |
| Generate visual comparisons (20 samples side-by-side) | â­â­ (2) | 2.4 | 5 min | â³ |
| Export to TorchScript (.pt) | â­â­ (2) | 2.4 | 3 min | â³ |
| Export to ONNX (.onnx) | â­â­ (2) | 2.4 | 3 min | â³ |
| Update `model_registry.json` with version, metrics, file path | â­ (1) | 2.4 | 3 min | â³ |

**Evaluation Decision Tree** (CC = 5):
```python
def evaluate_model(checkpoint):
    metrics = compute_metrics(checkpoint)  # CC+1

    if metrics['iou'] > 0.90:  # CC+1
        print("PRODUCTION READY")
        return APPROVED
    elif metrics['iou'] > 0.85:  # CC+1
        print("ACCEPTABLE, MONITOR")
        return CONDITIONAL
    else:  # CC+1
        print("REJECT, RETRAIN")
        return REJECTED  # CC+1
```

---

### Task 2.6: Cost Minimization
**Complexity**: â­ (CC = 1)
**ROI**: ğŸ”¥ğŸ”¥ğŸ”¥ (saves $50+ per training session)
**Effort**: 5 min (at end)
**Dependencies**: Task 2.5 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Download model to local machine | â­ (1) | 2.5 | 2 min | â³ |
| Terminate GPU instance immediately | â­ (1) | 2.5 | 1 min | â³ |
| Verify termination in provider console | â­ (1) | 2.5 | 2 min | â³ |

---

## Summary: Phase 2 (Light Finetune)

```
Total Effort: 4-6 hours (90 min GPU, rest CPU)
Total Cost: $1.25 GPU hours Ã— $0.75/hr = $0.94 (Lambda Labs)
Max Complexity: CC=12 (training loop)
Critical Path: 2.1 â†’ 2.2 â†’ 2.3 â†’ 2.4 â†’ 2.5 â†’ 2.6
Parallel Tasks: 2.2 (while 2.1 runs), 2.3 (while 2.1 runs)
ROI: ğŸ”¥ğŸ”¥ (improves quality 5-10%, still very cheap)
```

**Success Criteria**:
- âœ… Val IoU >0.90 (production-ready)
- âœ… Inference <500ms CPU (acceptable)
- âœ… Training time <120 min (cost-controlled)
- âœ… Checkpoints exported and versioned

---

## PHASE 3: Full Training (RARE)

### Overview
- **Status**: ğŸ”´ Only if Phase 2 still insufficient (IoU <0.90)
- **Cost**: $50-200 (10-20 GPU hours)
- **Time**: 20-40 hours
- **Risk**: High (expensive, long)
- **ROI**: Low (diminishing returns)
- **When**: Very rare, only if extreme quality needed

### Task 3.1: Dataset Expansion
**Complexity**: â­â­ (CC = 4)
**Effort**: 2-3 hours
**Dependencies**: Phase 2 complete

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Collect additional datasets (CelebA-HQ, LFW, custom) | â­â­ (2) | None | 60 min | â³ |
| Annotate custom images (manual hair masks) | â­â­â­ (3) | None | 90 min | â³ |
| Merge and deduplicate (check for overlaps) | â­â­ (2) | None | 15 min | â³ |
| Create 5000+ sample training set | â­ (1) | None | 5 min | â³ |

---

### Task 3.2: Training from Scratch
**Complexity**: â­â­â­â­ (CC = 15)
**Effort**: 15-20 GPU hours
**Dependencies**: Task 3.1 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Train DeepLabV3+ from scratch (full backbone) | â­â­â­â­ (15) | 3.1 | 600-1200 min | â³ |
| Monitor loss curves and validation metrics | â­â­ (2) | 3.1 | 30 min | â³ |
| Implement learning rate decay and warmup | â­â­â­ (3) | 3.1 | 30 min | â³ |

---

### Task 3.3: Post-Training Analysis
**Complexity**: â­â­â­ (CC = 7)
**Effort**: 2-3 hours
**Dependencies**: Task 3.2 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Hyperparameter sensitivity analysis | â­â­â­ (3) | 3.2 | 60 min | â³ |
| Error analysis (worst cases) | â­â­ (2) | 3.2 | 30 min | â³ |
| Model compression & quantization | â­â­â­ (4) | 3.2 | 30 min | â³ |

---

## PHASE 4: Integration & Production Deployment

### Overview
- **Status**: ğŸŸ¡ Ready after Phase 1 or Phase 2 complete
- **Cost**: $0 (deployment only)
- **Time**: 2-3 hours
- **Risk**: Low (rollback plan in place)
- **ROI**: ğŸ”¥ğŸ”¥ (enables production use)

### Task 4.1: Model Versioning & Registry
**Complexity**: â­ (CC = 2)
**Effort**: 15 min
**Dependencies**: Phase 1 complete (Task 1.6) OR Phase 2 complete (Task 2.5)

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Create `model_registry.json` with version, metrics, metadata | â­ (1) | 1.6 or 2.5 | 5 min | â³ |
| Store model in Git LFS or S3 with checksums | â­ (1) | 1.6 or 2.5 | 5 min | â³ |
| Document training command and hyperparameters | â­ (1) | 1.6 or 2.5 | 5 min | â³ |

---

### Task 4.2: Update segmenter.py with Production Model
**Complexity**: â­â­ (CC = 5)
**Effort**: 30 min
**Dependencies**: Task 4.1 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Replace stub segment_hair() with real model inference | â­â­ (2) | 4.1 | 10 min | â³ |
| Update ModelCache.segmenter() to load production model | â­â­ (3) | 4.1 | 10 min | â³ |
| Test integration: image â†’ mask â†’ output | â­ (1) | 4.1 | 5 min | â³ |
| Add version logging to every request | â­â­ (2) | 4.1 | 5 min | â³ |

---

### Task 4.3: A/B Testing Setup
**Complexity**: â­â­â­ (CC = 8)
**Effort**: 45 min
**Dependencies**: Task 4.2 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Implement feature flag (10% â†’ 50% â†’ 100% rollout) | â­â­ (2) | 4.2 | 15 min | â³ |
| Setup metrics collection (response time, error rate, IoU) | â­â­â­ (3) | 4.2 | 15 min | â³ |
| Define rollback thresholds (error >2%, latency p95 >1000ms) | â­ (1) | 4.2 | 5 min | â³ |
| Test rollback procedure (git revert + redeploy) | â­â­ (2) | 4.2 | 10 min | â³ |

**A/B Testing Decision Tree** (CC = 8):
```python
def should_rollout(metrics):
    if metrics['error_rate'] > 0.02:  # CC+1
        return ROLLBACK  # CC+1
    elif metrics['latency_p95'] > 1000:  # CC+1
        return ROLLBACK  # CC+1
    elif current_traffic == 0.10:  # CC+1
        return INCREASE_TO_50  # CC+1
    elif current_traffic == 0.50:  # CC+1
        return INCREASE_TO_100  # CC+1
    elif current_traffic == 1.00:  # CC+1
        return MONITOR  # CC+1
    else:
        return HOLD  # CC+1
```

---

### Task 4.4: Monitoring & Alerting
**Complexity**: â­â­â­ (CC = 7)
**Effort**: 30 min
**Dependencies**: Task 4.3 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Setup TensorBoard dashboard for inference metrics | â­â­ (2) | 4.3 | 10 min | â³ |
| Create alerts for error rate >2%, latency p95 >1000ms | â­â­â­ (3) | 4.3 | 10 min | â³ |
| Log segmentation failures for categorization (edge cases) | â­â­ (2) | 4.3 | 5 min | â³ |
| Setup email/Slack notification on threshold breach | â­â­ (2) | 4.3 | 5 min | â³ |

---

### Task 4.5: Documentation & Handoff
**Complexity**: â­ (CC = 1)
**Effort**: 20 min
**Dependencies**: Task 4.4 âœ…

| Subtask | Complexity | Deps | Time | Status |
|---------|------------|------|------|--------|
| Update README.md with deployed model version + performance | â­ (1) | 4.4 | 5 min | â³ |
| Document rollback procedure in CONTRIBUTING.md | â­ (1) | 4.4 | 5 min | â³ |
| Create incident response playbook (what if fails) | â­â­ (2) | 4.4 | 10 min | â³ |

---

## Summary: Phase 4 (Integration)

```
Total Effort: 2-3 hours
Total Cost: $0 (deployment only)
Max Complexity: CC=8 (A/B testing)
Critical Path: 4.1 â†’ 4.2 â†’ 4.3 â†’ 4.4 â†’ 4.5
Parallel Tasks: 4.3, 4.4 (after 4.2)
ROI: ğŸ”¥ğŸ”¥ (enables production use, risk-controlled)
```

---

## ğŸ“Š Cross-Phase Dependency Graph

```
PHASE 1 (Zero-Train)          PHASE 2 (Optional Finetune)      PHASE 3 (Rare)      PHASE 4 (Deploy)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1.1 Environment
 â†“
1.2 MediaPipe Integration      2.1 Dataset Prep â”€â”€â”€â”€â”€â”€â”
 â†“                               â†“                     â”œâ”€â†’ 2.4 Training
1.3 Fixture Testing            2.2 GPU Setup â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â†“                               â†“                     â”‚
1.4 Post-Processing            2.3 Model Select â”€â”€â”€â”€â”€â”˜
 â†“
1.5 Benchmarking                              2.5 Export
 â†“                                             â†“
1.6 Deploy to Staging  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    4.1 Versioning
                                             â†“
                                           4.2 Integration
                                             â†“
                                           4.3 A/B Testing
                                             â†“
                                           4.4 Monitoring
                                             â†“
                                           4.5 Documentation
```

---

## ğŸ“ˆ ROI Analysis by Phase

### Phase 1 (Zero-Train) â€” HIGHEST ROI
- **Cost**: $0
- **Time**: 2 hours
- **Quality**: 4/5 (MediaPipe is state-of-the-art)
- **ROI Score**: ğŸ”¥ğŸ”¥ğŸ”¥ (infinite)
- **Decision**: ALWAYS START HERE

### Phase 2 (Light Finetune) â€” MEDIUM ROI
- **Cost**: $1-2
- **Time**: 4-6 hours
- **Quality**: 4.5-5/5 (custom optimized)
- **ROI Score**: ğŸ”¥ğŸ”¥ (3x-4x quality improvement per $)
- **Decision**: Only if Phase 1 IoU <0.85

### Phase 3 (Full Training) â€” LOW ROI
- **Cost**: $50-200
- **Time**: 20-40 hours
- **Quality**: 5/5 (unlimited)
- **ROI Score**: ğŸ”¥ (1.01x quality improvement per $)
- **Decision**: AVOID unless extreme quality needed

### Phase 4 (Integration) â€” MEDIUM ROI
- **Cost**: $0
- **Time**: 2-3 hours
- **Quality**: N/A (deployment)
- **ROI Score**: ğŸ”¥ğŸ”¥ (enables production)
- **Decision**: REQUIRED after Phase 1 or 2

---

## ğŸ¯ Cyclomatic Complexity Summary

| Phase | Task | CC | Risk | Testability |
|-------|------|-----|------|-------------|
| 1 | 1.1 Setup | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 1 | 1.2 Integration | 5 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 1 | 1.3 Testing | 6 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 1 | 1.4 Post-Processing | 8 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 1 | 1.5 Benchmarking | 2 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 1 | 1.6 Deployment | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 2 | 2.1 Data Prep | 4 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 2 | 2.2 GPU Setup | 2 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 2 | 2.3 Model Selection | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 2 | 2.4 Training | 12 | ğŸ”´ High | ğŸŸ¡ Fair |
| 2 | 2.5 Export | 5 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 2 | 2.6 Cost Minimization | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 3 | 3.1 Dataset Expansion | 4 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 3 | 3.2 Full Training | 15 | ğŸ”´ High | ğŸŸ¡ Fair |
| 3 | 3.3 Analysis | 7 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 4 | 4.1 Versioning | 2 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |
| 4 | 4.2 Integration | 5 | ğŸŸ¡ Low | ğŸŸ¡ Good |
| 4 | 4.3 A/B Testing | 8 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 4 | 4.4 Monitoring | 7 | ğŸŸ¡ Medium | ğŸŸ¡ Fair |
| 4 | 4.5 Documentation | 1 | ğŸŸ¢ Minimal | ğŸŸ¢ Excellent |

**Key Insights**:
- **Low CC (1-2)**: Setup, deployment, cost control â€” easy, test with simple unit tests
- **Medium CC (4-8)**: Integration, testing, monitoring â€” moderate, need branch coverage
- **High CC (12-15)**: Training loops â€” complex, need integration tests + visual validation

---

## ğŸš¦ Recommended Execution Path

### Week 1: Phase 1 (Zero-Train)
```
Day 1 (Mon-Tue):    Tasks 1.1-1.2 (env + integration)
Day 2 (Wed):        Tasks 1.3-1.4 (testing + post-processing)
Day 3 (Thu):        Tasks 1.5-1.6 (benchmarking + deploy)

Effort: 2-2.5 hours total
Cost: $0
Risk: ğŸŸ¢ Minimal
Result: Production-ready MediaPipe baseline
```

### Week 2: Phase 2 (IF NEEDED)
```
Only if Phase 1 IoU <0.85 on edge cases

Day 1 (Mon-Tue):    Tasks 2.1-2.3 (prep + GPU setup)
Day 2-3 (Wed-Thu):  Task 2.4 (training, run overnight)
Day 4 (Fri):        Tasks 2.5-2.6 (export + shutdown)

Effort: 4-6 hours total (including GPU time)
Cost: $1-2
Risk: ğŸŸ¡ Low
Result: Improved model v1.1 with better edge case handling
```

### Week 3: Phase 4 (After Phase 1 or 2)
```
Day 1 (Mon):        Tasks 4.1-4.2 (versioning + integration)
Day 2 (Tue):        Task 4.3 (A/B testing setup)
Day 3 (Wed):        Task 4.4 (monitoring)
Day 4 (Thu):        Task 4.5 (documentation)

Effort: 2-3 hours total
Cost: $0
Risk: ğŸŸ¢ Minimal
Result: Production deployment with safety rollback
```

---

**Document Version**: 1.0 (Phases Breakdown)
**Last Updated**: 2025-12-20
**Owner**: ML Team
**Review Cycle**: After each phase completion
